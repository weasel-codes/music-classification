{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Music-Genre-Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1unrXBTlvjiA_OFVpjSVqAg2aO_Ujoe9k",
      "authorship_tag": "ABX9TyPd1iRbNISMAEmEchcy3pha",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weasel-codes/music-classification/blob/main/Music_Genre_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2E7jylWWhuN"
      },
      "source": [
        "# DATASET CREATION\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KheYN2jH6tw5"
      },
      "source": [
        "Extracts MFCCs from music dataset and saves them into a json file along with genre labels.\n",
        "\n",
        "Different variables used here are :\n",
        "\n",
        "    :param dataset_path (str): Path to dataset\n",
        "\n",
        "    :param json_path (str): Path to json file used to save MFCCs\n",
        "\n",
        "    :param num_mfcc (int): Number of coefficients to extract\n",
        "    \n",
        "    :param n_fft (int): Interval we consider to apply FFT. Measured in # of samples\n",
        "\n",
        "    :param hop_length (int): Sliding window for FFT. Measured in # of samples\n",
        "\n",
        "    :param: num_segments (int): Number of segments we want to divide sample tracks into\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UotaDdYf5Du8"
      },
      "source": [
        "# LIBRARIES\n",
        "import json\n",
        "import os\n",
        "import math\n",
        "import librosa"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJ3w8-Fc5Qdc"
      },
      "source": [
        "#CONSTANTS\n",
        "DATASET_PATH = \"/content/drive/MyDrive/ML/songs\"\n",
        "JSON_PATH = \"/content/drive/MyDrive/ML/MFCC.json\"\n",
        "\n",
        "SAMPLE_RATE = 22050\n",
        "TRACK_DURATION = 30 # measured in seconds\n",
        "SAMPLES_PER_TRACK = SAMPLE_RATE * TRACK_DURATION # TOTAL samples from a track\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KleCk1HN5cAm"
      },
      "source": [
        "# PROCESS WACH SEGMENT OF AUDIO FILE SEPERATELY\n",
        "def processEachSegmentOfFile(segment_num, data, file_path, curr_signal, sample_rate, num_mfcc, n_fft, hop_length, num_mfcc_vectors_per_segment, label_index):\n",
        "    mfcc = librosa.feature.mfcc(curr_signal, sample_rate, n_mfcc=num_mfcc, n_fft=n_fft, hop_length=hop_length) # EXTRACT MFCC\n",
        "    mfcc = mfcc.T\n",
        "    if len(mfcc) == num_mfcc_vectors_per_segment:     # store only mfcc feature with expected number of vectors\n",
        "        data[\"mfcc\"].append(mfcc.tolist())\n",
        "        data[\"labels\"].append(label_index)\n",
        "        print(\"{}, segment:{}\".format(file_path, segment_num+1))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmsiqsJh5B1m"
      },
      "source": [
        "# PROCESS EACH AUDIO FILES SEPARATELY\n",
        "def processEachAudioFile(data, dirpath, file_name, num_segments, samples_per_segment, num_mfcc, n_fft, hop_length, num_mfcc_vectors_per_segment, label_index):\n",
        "    file_path = os.path.join(dirpath, file_name)  # load audio file from FULL PATH + FILE NAME\n",
        "    signal, sample_rate = librosa.load(file_path, sr=SAMPLE_RATE)\n",
        "\n",
        "    for d in range(num_segments):             #process all segments of audio file : DIVIDED one file into MULTIPLE SEGMENTS\n",
        "        start = samples_per_segment * d       #Start time of current segment\n",
        "        finish = start + samples_per_segment  #Finish Time of segment\n",
        "\n",
        "        curr_signal = signal[start:finish]    #Extract required samples out of signal\n",
        "        processEachSegmentOfFile(d, data, file_path, curr_signal, sample_rate, num_mfcc, n_fft, hop_length, num_mfcc_vectors_per_segment, label_index)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkalPIn789I5"
      },
      "source": [
        "# SAVE MFCC-INFO CARRYING DATA-JSON TO A FILE ON DRIVE\n",
        "def saveDataToJson(data):\n",
        "    with open(JSON_PATH, \"w\") as fp:\n",
        "        json.dump(data, fp, indent=4)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4bCkvLEWkik"
      },
      "source": [
        "# Dictionary to store mapping, labels, and MFCCs\n",
        "data = {\n",
        "    \"mapping\": [],  #Relate to labels\n",
        "    \"labels\": [],   #Output\n",
        "    \"mfcc\": []      #Training Input\n",
        "}\n",
        "\n",
        "# PROCESS EACH GENRE\n",
        "def save_mfcc(num_mfcc, n_fft, hop_length, num_segments):\n",
        "    samples_per_segment = int(SAMPLES_PER_TRACK / num_segments)\n",
        "    num_mfcc_vectors_per_segment = math.ceil(samples_per_segment / hop_length)\n",
        "\n",
        "    # dirpath : folder we are currently in, dirnames : sub-folders or genre, filenames : each file of a genre\n",
        "    for i, (dirpath, dirnames, filenames) in enumerate(os.walk(DATASET_PATH)): # loop through all genre sub-folder\n",
        "\n",
        "        label_index = i-1\n",
        "        # ensure we're processing a genre sub-folder level : to make sure we are inside some genre : genre level and not dataset level\n",
        "        if dirpath is not DATASET_PATH:\n",
        "\n",
        "            # save genre label (i.e., sub-folder name) in the mapping\n",
        "            semantic_label = dirpath.split(\"/\")[-1]\n",
        "            data[\"mapping\"].append(semantic_label) # No need if we already know what all labels we have\n",
        "            print(\"\\nProcessing: {}\".format(semantic_label))\n",
        "\n",
        "            for file_name in filenames: # Procesing each file\n",
        "                processEachAudioFile(data, dirpath, file_name, num_segments, samples_per_segment, num_mfcc, n_fft, hop_length, num_mfcc_vectors_per_segment, label_index)\n",
        "\n",
        "    saveDataToJson(data)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPEV8fog9uNP"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    num_mfcc=13\n",
        "    n_fft=2048\n",
        "    hop_length=512\n",
        "    num_segments=5\n",
        "    save_mfcc(num_mfcc, n_fft, hop_length, num_segments)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}